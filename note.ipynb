{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f6cf8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baa4497c",
   "metadata": {},
   "source": [
    "# RLHF\n",
    "created by cyj\n",
    "## 1ã€PG\n",
    "### 1ã€PGçš„ä¼˜åŒ–ç›®æ ‡\n",
    "æˆ‘ä»¬å¸Œæœ›å­¦ä¹ ä¸€ä¸ªå‚æ•°åŒ–çš„ç­–ç•¥$\\pi_\\theta(a \\mid s)$ï¼Œä½¿å¾—æœŸæœ›å›æŠ¥æœ€å¤§åŒ–ï¼š\n",
    "$$\n",
    "J(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[ R(\\tau) \\right]\n",
    "$$\n",
    "\n",
    "- $\\theta$ è¡¨ç¤ºç­–ç•¥ç½‘ç»œå‚æ•°\n",
    "\n",
    "- $\\tau = (s_0, a_0, s_1, a_1, \\ldots)$ è¡¨ç¤ºç”±ç­–ç•¥ $\\pi_\\theta$ é‡‡æ ·å¾—åˆ°çš„è½¨è¿¹\n",
    "\n",
    "- $R(\\tau)$ è¡¨ç¤ºè½¨è¿¹çš„ç´¯è®¡å›æŠ¥ã€‚  \n",
    "ç­–ç•¥æ¢¯åº¦ä¸æ˜¯åœ¨â€œå­¦å¯¹çš„åŠ¨ä½œâ€ï¼Œè€Œæ˜¯åœ¨â€œæé«˜äº§ç”Ÿå¥½ç»“æœçš„æ¦‚ç‡â€  \n",
    "### 2ã€PGçš„æ ¸å¿ƒå…¬å¼\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; G_t\n",
    "\\right]\n",
    "$$\n",
    "- $G_t = \\sum_{t'=t}^{T} r_{t'}$ è¡¨ç¤ºä»æ—¶é—´æ­¥ $t$ å¼€å§‹çš„ç´¯è®¡å›æŠ¥ï¼ˆreturnï¼‰ã€‚  \n",
    "- $\\log \\pi_\\theta(a_t \\mid s_t)$ æ˜¯å·²é‡‡æ ·åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡   \n",
    "è¯¥å…¬å¼å°†ä¸å¯å¾®çš„ç¯å¢ƒå¥–åŠ±é—®é¢˜ï¼Œè½¬åŒ–ä¸ºå¯¹ $\\log \\pi_\\theta$ çš„å¯å¾®ä¼˜åŒ–ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8b7b6",
   "metadata": {},
   "source": [
    "### 3ã€Returnçš„å®šä¹‰ï¼ˆMonte Carloï¼‰\n",
    "åœ¨æœ€åŸºæœ¬çš„REINFRORCEç®—æ³•ä¸­ï¼ŒReturnå®šä¹‰ä¸ºï¼š\n",
    "$$\n",
    "G_t\n",
    "=\n",
    "\\sum_{k=t}^{T}\n",
    "\\gamma^{k-t} r_k\n",
    "$$\n",
    "- $r_k$ æ˜¯æ—¶é—´æ­¥ $k$ çš„å³æ—¶å¥–åŠ±\n",
    "- $\\gamma \\in [0, 1]$ æ˜¯æŠ˜æ‰£å› å­ï¼Œæ§åˆ¶æœªæ¥å¥–åŠ±çš„é‡è¦æ€§\n",
    "è¿™ç§ä¼°è®¡æ–¹å¼æ˜¯æ— åä½†æ˜¯æ–¹å·®é«˜çš„ï¼š  \n",
    "Monte Carlo return æ˜¯æ— åä¼°è®¡ï¼Œå› ä¸ºå®ƒå®Œæ•´é‡‡æ ·äº†çœŸå®è½¨è¿¹çš„æ‰€æœ‰æœªæ¥å¥–åŠ±ï¼›\n",
    "ä½†å®ƒæ˜¯é«˜æ–¹å·®ä¼°è®¡ï¼Œå› ä¸ºåŒä¸€çŠ¶æ€ä¸‹çš„æœªæ¥è½¨è¿¹å…·æœ‰å·¨å¤§çš„éšæœºæ€§\n",
    "\n",
    "åœ¨ Policy Gradient ä¸­$G_t$ä¸æ˜¯å•æ­¥ rewardï¼Œè€Œæ˜¯â€œä»æ—¶é—´æ­¥ ğ‘¡å¼€å§‹çš„ç´¯è®¡å›æŠ¥ï¼ˆreturnï¼‰â€ï¼›å®ƒç”± reward ç»„æˆï¼Œå› æ­¤å¯ä»¥ç†è§£ä¸ºä¸€ç§â€œç»¼åˆ reward åˆ†æ•°â€ï¼Œä½†ä¸æ˜¯åŸå§‹ reward æœ¬èº«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a6f03",
   "metadata": {},
   "source": [
    "### 4ã€PGä¸­çš„Losså½¢å¼\n",
    "ç”±äºå®é™…è®­ç»ƒä¸­æˆ‘ä»¬é€šå¸¸æœ€å°åŒ–lossï¼Œå¯ä»¥å°†ç›®æ ‡å‡½æ•°å–è´Ÿï¼Œå¾—åˆ°ç­‰ä»·çš„losså½¢å¼ï¼š\n",
    "$$\n",
    "L^{PG}(\\theta)\n",
    "=\n",
    "-\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}\n",
    "\\log \\pi_\\theta(a_t \\mid s_t)\\; G_t\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "è¿™é‡Œå¯ä»¥ç†è§£ä¸ºï¼Œä»¥å›æŠ¥$G_t$ä¸ºæƒé‡ï¼Œå¯¹å·²é‡‡æ ·åŠ¨ä½œçš„ log-likelihood è¿›è¡Œæœ€å¤§åŒ–,è®©æ¨¡å‹ä»¥åæ›´å®¹æ˜“ç”Ÿæˆè¿™äº›åŠ¨ä½œ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb2dc2",
   "metadata": {},
   "source": [
    "### 5ã€Baseline ä¸ æ–¹å·®é™ä½\n",
    "å¯¹äºä»»æ„ä¸ä¾èµ–äºåŠ¨ä½œ$a_t$ çš„å‡½æ•° $b(s_t)$ï¼Œæœ‰ï¼š\n",
    "$$\n",
    "\\mathbb{E}_{a_t \\sim \\pi_\\theta}\n",
    "\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; b(s_t) \n",
    "\\right]\n",
    "=\n",
    "b(s_t)\\; \\mathbb{E}_{a_t \\sim \\pi_\\theta}\n",
    "\\left[\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\n",
    "\\right]\n",
    "= \n",
    "0\n",
    "$$\n",
    "å› æ­¤ï¼Œå¯ä»¥å°† $b(s_t)$ ä»æ¢¯åº¦ä¸­å‡å»ï¼Œè€Œä¸å¼•å…¥åå·®ï¼š\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; (G_t - b(s_t))\n",
    "\\right]\n",
    "$$\n",
    "- $b(s_t)$ ç§°ä¸º baselineï¼Œå¸¸ç”¨çš„é€‰æ‹©æ˜¯çŠ¶æ€å€¼å‡½æ•° $V^\\pi(s_t)$  \n",
    "- é€šè¿‡å‡å» baselineï¼Œå¯ä»¥æ˜¾è‘—é™ä½æ¢¯åº¦çš„æ–¹å·®ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§  \n",
    "- ç›´è§‚ä¸Šï¼Œbaseline èµ·åˆ°äº†â€œä¸­å¿ƒåŒ–â€å›æŠ¥çš„ä½œç”¨ï¼Œä½¿å¾—åªæœ‰å½“ $G_t$ é«˜äº baseline æ—¶ï¼Œæ‰ä¼šå¢åŠ è¯¥åŠ¨ä½œçš„æ¦‚ç‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012769a2",
   "metadata": {},
   "source": [
    "### 6ã€Advantage Function\n",
    "å½“baselineä¸ºçŠ¶æ€ä»·å€¼å‡½æ•°$b(s_t) = V^\\pi(s_t)$æ—¶ï¼Œå®šä¹‰ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰ä¸ºï¼š\n",
    "$$\n",
    "A^\\pi(s_t, a_t)\n",
    "=\n",
    "G_t - V^\\pi(s_t)\n",
    "$$\n",
    "æ­¤æ—¶ï¼ŒPolicy Gradientå…¬å¼å¯å†™ä¸ºï¼š\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; A^\\pi(s_t, a_t)\n",
    "\\right]\n",
    "$$\n",
    "Value function é¢„æµ‹å¦‚æœæˆ‘ä»è¿™é‡Œç»§ç»­èµ°ï¼Œå¤§æ¦‚èƒ½æ‹¿å¤šå°‘ reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fd2f8",
   "metadata": {},
   "source": [
    "### 7ã€çŠ¶æ€ä»·å€¼å‡½æ•°å¦‚ä½•è®¡ç®—\n",
    "çŠ¶æ€ä»·å€¼å‡½æ•°çš„ç›®çš„æ˜¯é™ä½æ–¹å·®ï¼Œ $V^\\pi(s_t)$ å¯ä»¥é€šè¿‡å¤šç§æ–¹æ³•ä¼°è®¡ï¼Œå¸¸è§çš„æ–¹æ³•åŒ…æ‹¬ï¼š\n",
    "- **è’™ç‰¹å¡æ´›ä¼°è®¡**ï¼šä½¿ç”¨å¤šæ¬¡é‡‡æ ·çš„å®Œæ•´è½¨è¿¹æ¥è®¡ç®—å¹³å‡å›æŠ¥ï¼ˆæœ€ç›´è§‚çš„æ–¹æ³•ï¼Œä½†æ˜¯æœ€å°‘ç”¨ï¼‰\n",
    "- **æ—¶åºå·®åˆ†å­¦ä¹ ï¼ˆTD Learningï¼‰**ï¼šä½¿ç”¨å•æ­¥å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä¼°è®¡å€¼æ¥æ›´æ–°å½“å‰çŠ¶æ€çš„ä»·å€¼ä¼°è®¡\n",
    "- **GAEï¼ˆGeneralized Advantage Estimationï¼‰**ï¼šç»“åˆå¤šæ­¥å›æŠ¥å’ŒTDè¯¯å·®ï¼Œå¹³è¡¡åå·®å’Œæ–¹å·®ï¼ˆPPO/A2Cçš„é€‰æ‹©ï¼‰\n",
    "- **å‡½æ•°é€¼è¿‘**ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œç­‰å‡½æ•°é€¼è¿‘å™¨æ¥å­¦ä¹ çŠ¶æ€ä»·å€¼å‡½æ•°ï¼Œé€šè¿‡æœ€å°åŒ–é¢„æµ‹å€¼ä¸å®é™…å›æŠ¥ä¹‹é—´çš„è¯¯å·®æ¥è®­ç»ƒç½‘ç»œï¼ˆæœ€å¸¸è§ï¼‰\n",
    "\n",
    "GAEçš„æ ¸å¿ƒå…¬å¼æ˜¯ï¼š\n",
    "$$\n",
    "A^{GAE(\\gamma, \\lambda)}_t\n",
    "=\n",
    "\\sum_{l=0}^{\\infty}\n",
    "(\\gamma \\lambda)^l \\delta_{t+l}\n",
    "$$\n",
    "å…¶ä¸­ï¼Œ$\\delta_t$ æ˜¯æ—¶åºå·®åˆ†è¯¯å·®ï¼Œå®šä¹‰ä¸ºï¼š\n",
    "$$\n",
    "\\delta_t\n",
    "=\n",
    "r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
    "$$\n",
    "- $\\lambda \\in [0, 1]$ æ˜¯ä¸€ä¸ªè°ƒèŠ‚å‚æ•°ï¼Œæ§åˆ¶åå·®ä¸æ–¹å·®çš„æƒè¡¡\n",
    "- r_t æ˜¯æ—¶é—´æ­¥ t çš„å³æ—¶å¥–åŠ±\n",
    "- $V(s_t)$ æ˜¯çŠ¶æ€ $s_t$ çš„ä»·å€¼ä¼°è®¡\n",
    "- å½“ $\\lambda = 0$ æ—¶ï¼ŒGAE é€€åŒ–ä¸ºå•æ­¥ TD è¯¯å·®\n",
    "- å½“ $\\lambda = 1$ æ—¶ï¼ŒGAE é€€åŒ–ä¸ºè’™ç‰¹å¡æ´›ä¼°è®¡\n",
    "ç›´è§‰å«ä¹‰å°±æ˜¯æˆ‘è¿™ä¸€æ­¥ä¹‹åä¹‹åï¼Œæ¯”æˆ‘åŸæ¥é¢„æœŸçš„æ›´å¥½è¿˜æ˜¯æ›´å·®\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fad755",
   "metadata": {},
   "source": [
    "æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ€»ç»“ä¸€ä¸‹å®Œæ•´çš„ä½¿ç”¨GAEä½œä¸ºä¼˜åŠ¿å‡½æ•°çš„Policy Gradientç®—æ³•æ­¥éª¤ï¼š  \n",
    "Step1:Value Networkå‰å‘ï¼Œè®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€ä»·å€¼ä¼°è®¡ $V(s_t)$  \n",
    "Step2:è®¡ç®—æ—¶åºå·®åˆ†è¯¯å·® $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ï¼Œå…¶ä¸­ $r_t$ æ˜¯æ—¶é—´æ­¥ $t$ çš„å³æ—¶å¥–åŠ±ï¼Œr_tæ˜¯é€šè¿‡ç¯å¢ƒåé¦ˆå¾—åˆ°çš„å¥–åŠ±\n",
    "Step3:è®¡ç®—GAEä¼˜åŠ¿ä¼°è®¡ï¼š\n",
    "$$\n",
    "A^{GAE(\\gamma, \\lambda)}_t\n",
    "=\n",
    "\\sum_{l=0}^{\\infty}\n",
    "(\\gamma \\lambda)^l \\delta_{t+l}\n",
    "$$ \n",
    "Step4:ä½¿ç”¨GAEä¼˜åŠ¿å‡½æ•°æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°ï¼š\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; A^{GAE(\\gamma, \\lambda)}_t\n",
    "\\right]\n",
    "$$\n",
    "Steap5:Value Networkçš„æ›´æ–°ï¼Œæœ€å°åŒ–ä»¥ä¸‹æŸå¤±å‡½æ•°ï¼š\n",
    "$$\n",
    "L^{V}(\\phi)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}\n",
    "\\left(V_\\phi(s_t) - G_t\\right)^2\n",
    "\\right]\n",
    "$$\n",
    "- å…¶ä¸­ï¼Œ$V_\\phi(s_t)$ æ˜¯å‚æ•°ä¸º $\\phi$ çš„ä»·å€¼ç½‘ç»œçš„ä¼°è®¡å€¼ï¼Œ$G_t$ æ˜¯ä»æ—¶é—´æ­¥ $t$ å¼€å§‹çš„ç´¯è®¡å›æŠ¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51e4dc",
   "metadata": {},
   "source": [
    "### 8ã€PGä¼ªä»£ç \n",
    "ä¸ç”¨GAEï¼Œåªç”¨Monte Carlo Returnï¼Œæœ‰policyï¼Œæœ‰valueï¼Œä¸ç”¨PPOçš„ratio/clip\n",
    "å…¬å¼å°±æ˜¯ï¼š\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; (G_t - V_\\phi(s_t))\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initialize policy network Ï€Î¸(a | s)\n",
    "Initialize value network VÏ†(s)\n",
    "\n",
    "for iteration = 1, 2, ... do:\n",
    "    Collect trajectories using current policy Ï€Î¸\n",
    "\n",
    "    for each trajectory:\n",
    "        for each timestep t:\n",
    "            compute return:\n",
    "                G_t = sum_{k=t}^T Î³^{k-t} r_k\n",
    "            compute advantage:\n",
    "                A_t = G_t - VÏ†(s_t)\n",
    "\n",
    "    Policy update:\n",
    "        L_policy = - E[ log Ï€Î¸(a_t | s_t) * A_t ]\n",
    "        update Î¸ by gradient descent\n",
    "\n",
    "    Value update:\n",
    "        L_value = E[ (VÏ†(s_t) - G_t)^2 ]\n",
    "        update Ï† by gradient descent\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Policy Model\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logits = nn.Parameter(torch.zeros(2))  # åˆå§‹åŒ–ä¸¤ä¸ªåŠ¨ä½œçš„logits\n",
    "        \n",
    "    def forward(self):\n",
    "        return F.softmax(self.logits, dim=-1)  # dim=-1åœ¨softmaxä¸­è¡¨ç¤ºå¯¹æ¯ä¸€è¡Œçš„ç±»åˆ«æ¦‚ç‡è¿›è¡Œè®¡ç®—\n",
    "\n",
    "# ä»·å€¼ç½‘ç»œ\n",
    "# class ValueNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.value = nn.Parameter(torch.zeros(1))  # ä»·å€¼å‡½æ•°ï¼Œåªæœ‰ä¸€ä¸ªæ•°å€¼\n",
    "        \n",
    "#     def forward(self):\n",
    "#         return self.value\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        # å®šä¹‰ä¸€ä¸ªä¸¤å±‚çš„ç®€å•ç¥ç»ç½‘ç»œ\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # ç¬¬ä¸€å±‚\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)  # è¾“å‡ºå±‚ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡ï¼ˆçŠ¶æ€çš„ä»·å€¼ä¼°è®¡ï¼‰\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°\n",
    "        return self.fc2(x)  # è¾“å‡ºæœ€ç»ˆçš„ä»·å€¼\n",
    "\n",
    "\n",
    "# å®šä¹‰ç¯å¢ƒ\n",
    "class SimpleEnv:\n",
    "    def __init__(self):\n",
    "        self.action = [0, 1]  # ä¸¤ä¸ªåŠ¨ä½œï¼Œ0å’Œ1\n",
    "        self.reward = [1, -1]  # æ¯ä¸ªåŠ¨ä½œçš„å¥–åŠ±\n",
    "    \n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        # æ ¹æ®åŠ¨ä½œè¿”å›å¥–åŠ±ï¼Œè¿™é‡Œç›´æ¥è¿”å›å¯¹åº”çš„å¥–åŠ±\n",
    "        reward = self.reward[action]\n",
    "        return reward\n",
    "\n",
    "def train(env, policy_net, value_net, optimizer, num_episodes=1000, gamma=0.99):\n",
    "    total_rewards = []  # å­˜å‚¨æ¯ä¸ªå›åˆçš„æ€»å¥–åŠ±\n",
    "    logits_history = []  # å­˜å‚¨logitsçš„å˜åŒ–\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor([state], dtype=torch.float32)  # è½¬æ¢ä¸ºtensor\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # è·å–ç­–ç•¥ç½‘ç»œçš„è¾“å‡º\n",
    "            action_probs = policy_net()\n",
    "            dist = torch.distributions.Categorical(action_probs)  # è®¡ç®—åŠ¨ä½œåˆ†å¸ƒ\n",
    "            action = dist.sample()  # ä»è®¡ç®—çš„åŠ¨ä½œåˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªåŠ¨ä½œ\n",
    "            log_prob = dist.log_prob(action)  # è®¡ç®—è¯¥åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡\n",
    "            \n",
    "            reward = env.step(action.item())  # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å¾—å¥–åŠ±\n",
    "            value = value_net(state)  # ç°åœ¨ä½¿ç”¨çŠ¶æ€è¾“å…¥æ¥è®¡ç®—å½“å‰çš„ä»·å€¼ä¼°è®¡\n",
    "            \n",
    "            td_target = reward + gamma * value\n",
    "            td_error = td_target - value\n",
    "            \n",
    "            # è®¡ç®—ç­–ç•¥æŸå¤±ï¼Œä»è®¡ç®—å›¾ä¸­åˆ†ç¦»TDè¯¯å·®ï¼Œé˜²æ­¢å½±å“åˆ°actorç½‘ç»œ\n",
    "            actor_loss = -log_prob * td_error.detach() \n",
    "            \n",
    "            # è®¡ç®—valueæŸå¤±ï¼ŒTDè¯¯å·®çš„å¹³æ–¹\n",
    "            critic_loss = td_error.pow(2)\n",
    "            \n",
    "            # æ€»æŸå¤±\n",
    "            loss = actor_loss + critic_loss  # æ€»æŸå¤± = ç­–ç•¥æŸå¤± + ä»·å€¼æŸå¤±\n",
    "            \n",
    "            # åå‘ä¼ æ’­\n",
    "            optimizer.zero_grad()  # æ¸…é™¤æ—§çš„æ¢¯åº¦\n",
    "            loss.backward()  # è®¡ç®—æ–°çš„æ¢¯åº¦\n",
    "            \n",
    "            # æ›´æ–°ç½‘ç»œå‚æ•°\n",
    "            optimizer.step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            done = True  # æ¯ä¸ªå›åˆç»“æŸä¹‹ååœæ­¢è®­ç»ƒ\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        logits_history.append(policy_net.logits.detach().numpy())  # è®°å½•logitsçš„å˜åŒ–\n",
    "\n",
    "    return total_rewards, logits_history\n",
    "\n",
    "\n",
    "# 5. åˆå§‹åŒ–ç¯å¢ƒã€æ¨¡å‹å’Œä¼˜åŒ–å™¨\n",
    "env = SimpleEnv()  # åˆå§‹åŒ–ç¯å¢ƒ\n",
    "policy_net = PolicyNet()  # åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰\n",
    "value_net = ValueNet()  # åˆå§‹åŒ–ä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰\n",
    "optimizer = optim.Adam(list(policy_net.parameters()) + list(value_net.parameters()), lr=0.01)  # ä½¿ç”¨Adamä¼˜åŒ–å™¨\n",
    "\n",
    "# 6. å¼€å§‹è®­ç»ƒå¹¶è®°å½•ç»“æœ\n",
    "total_rewards, logits_history = train(env, policy_net, value_net, optimizer)\n",
    "\n",
    "# 7. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹\n",
    "# ç»˜åˆ¶æ¯ä¸ªå›åˆçš„æ€»å¥–åŠ±\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_rewards)\n",
    "plt.title('Total Rewards over Episodes')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n",
    "\n",
    "# ç»˜åˆ¶ç­–ç•¥ç½‘ç»œçš„logitså˜åŒ–\n",
    "logits_history = torch.tensor(logits_history)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(logits_history[:, 0], label='Logits for Action 0')\n",
    "plt.plot(logits_history[:, 1], label='Logits for Action 1')\n",
    "plt.title('Logits for Actions over Episodes')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Logits')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb7b91",
   "metadata": {},
   "source": [
    "# 2ã€PPO\n",
    "PPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦ï¼Œæ¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œé˜²æ­¢ç­–ç•¥å‘ç”Ÿå‰§çƒˆå˜åŒ–ã€‚  \n",
    "PPOçš„ç›®æ ‡å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n",
    "$$\n",
    "L^{PPO}(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{old}}}\n",
    "\\left[\n",
    "\\min \\left(\n",
    "r_t(\\theta) A_t,\n",
    "\\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_{old}}(a_t \\mid s_t)}$ æ˜¯æ–°æ—§ç­–ç•¥çš„æ¦‚ç‡æ¯”ç‡\n",
    "- $A_t$ æ˜¯ä¼˜åŠ¿å‡½æ•°ï¼Œé€šå¸¸ä½¿ç”¨GAEä¼°è®¡\n",
    "- $\\epsilon$ æ˜¯ä¸€ä¸ªå°çš„è¶…å‚æ•°ï¼Œæ§åˆ¶è£å‰ªèŒƒå›´ï¼Œé€šå¸¸å–å€¼ä¸º0.1æˆ–0.2\n",
    "\n",
    "PPOå¦å¤–çš„æ ¸å¿ƒæ”¹è¿›çš„æ˜¯Minibatchå’Œå¤šè½®æ›´æ–°ï¼š\n",
    "- åœ¨æ¯æ¬¡ç­–ç•¥æ›´æ–°æ—¶ï¼ŒPPOä¼šä»é‡‡æ ·çš„è½¨è¿¹ä¸­éšæœºæŠ½å–å¤šä¸ªå°æ‰¹é‡ï¼ˆminibatchï¼‰è¿›è¡Œå¤šè½®ä¼˜åŒ–\n",
    "- è¿™ç§æ–¹å¼æé«˜äº†æ•°æ®åˆ©ç”¨ç‡ï¼Œå‡å°‘äº†æ–¹å·®ï¼Œä½¿å¾—è®­ç»ƒæ›´åŠ ç¨³å®šå’Œé«˜æ•ˆ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56879f",
   "metadata": {},
   "source": [
    "# 3ã€GRPO\n",
    "**motivation**ï¼š\n",
    "- ä»PGåˆ°PPOï¼Œéƒ½éœ€è¦è®­ç»ƒä¸€ä¸ªä¸policy modelå¤§å°ç›¸å½“å¤§critic modelæ¥ä¼°è®¡value functionï¼Œè¿™åœ¨å¤§æ¨¡å‹æ—¶ä»£æ˜¯éå¸¸ä¸ç°å®çš„ã€‚\n",
    "- æ­¤å¤–ï¼Œåœ¨LLMæ¨ç†ä»»åŠ¡ä¸­ï¼Œrewardé€šå¸¸åªä¼šåœ¨ç”Ÿæˆåºåˆ—çš„æœ«å°¾ç»™å‡ºï¼Œä¾‹å¦‚ï¼Œæœ€ç»ˆçš„ç­”æ¡ˆæ˜¯ä¸æ˜¯æ­£ç¡®ï¼Œè¿™ä½¿å¾—ä¸ºæ¯ä¸€ä¸ªtokenè®­ç»ƒä¸€ä¸ªç²¾å‡†çš„value functionå˜å¾—æ›´åŠ å›°éš¾ã€‚  \n",
    "\n",
    "**æ ¸å¿ƒåˆ›æ–°**ï¼š\n",
    "- æ”¾å¼ƒè®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„critic modelï¼Œè½¬è€Œç›´æ¥ä½¿ç”¨reward signalæ¥è®¡ç®—ä¼˜åŠ¿å‡½æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸€ä¸ªpromptï¼Œpolicy modelä¼šä½¿ç”¨æ—§çš„ç­–ç•¥é‡‡æ ·ç”Ÿæˆä¸€ä¸ªåŒ…å«Gä¸ªä¸åŒå›ç­”çš„groupï¼Œé€šè¿‡æ¯”è¾ƒè¿™ä¸ªç»„å„ä¸ªå›ç­”çš„å¥–åŠ±ï¼Œæ¥ä¼°è®¡æ¯ä¸ªå›ç­”çš„advantageã€‚  \n",
    "\n",
    "GRPOçš„ç›®æ ‡å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n",
    "$$\n",
    "L^{GRPO}(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta_{old}}}\n",
    "\\left[\n",
    "\\min \\left(\n",
    "r_t(\\theta) A^{GR}_t,\n",
    "\\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A^{GR}_t\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "- å…¶ä¸­ï¼Œ$A^{GR}_t$ æ˜¯åŸºäºgroup rewardè®¡ç®—çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå…·ä½“è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š\n",
    "$$\n",
    "A^{GR}_i\n",
    "=(r_i - mean({{r_j}^G_{j=1}}))/std({r_j}^G_{j=1})\n",
    "$$\n",
    "- $R_i$ æ˜¯ç¬¬iä¸ªå›ç­”çš„rewardï¼Œ$G$æ˜¯groupä¸­å›ç­”çš„æ•°é‡\n",
    "\n",
    "GRPOä¸­ï¼Œreward æ˜¯ é’ˆå¯¹æ•´ä¸ªåºåˆ—ç»™å‡ºçš„ï¼Œadvantageä¹Ÿæ˜¯é’ˆå¯¹æ•´ä¸ªåºåˆ—ç»™å‡ºçš„\n",
    "ä½†æ˜¯åœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œä»ç„¶æ˜¯å¯¹æ¯ä¸ªæ—¶é—´æ­¥ï¼ˆtokenï¼‰çš„ log-likelihood è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œè¿™æ˜¯ä¸ºäº†ç¡®ä¿Advantageè¿™ä¸ªå…¨å±€å¥–åŠ±èƒ½å¹³æ‘Šåˆ°æ¯ä¸€å¸§åŠ¨ä½œä¸Šï¼ˆTokenï¼‰ï¼Œä¸ä¼šå› ä¸ºåŠ¨ä½œæ—¶é—´é•¿å°±å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼š\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\n",
    "\\left[\n",
    "\\sum_{t}    \n",
    "\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)\\; A_t\n",
    "\\right]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58cbfe",
   "metadata": {},
   "source": [
    "åŸè®ºæ–‡ä¸­ä¸ºäº†è®­ç»ƒç¨³å®šæ€§ï¼Œä½¿ç”¨äº†ä¸€ä¸ªæ— å KL ä¼°è®¡å™¨ï¼ˆé€šå¸¸è¢«ç§°ä¸º K3 ä¼°è®¡å™¨çš„ä¸€ä¸ªå˜ç§ï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b130d9",
   "metadata": {},
   "source": [
    "# 4 ã€Dr.GRPO  \n",
    "1ã€å›ç­”çº§åˆ«çš„é•¿åº¦åå·®ï¼Œæºäºç›®æ ‡å‡½æ•°ä¸­çš„1/oiåƒï¼Œå½“Advantageä¸ºæ­£æ—¶ï¼Œè¿™ä¸ªå› å­ä¼šç»™äºˆè¾ƒçŸ­çš„æ­£ç¡®å›ç­”æ›´å¤§çš„æ¢¯åº¦æ›´æ–°ï¼Œé¼“åŠ±æ¨¡å‹ç”Ÿæˆç®€çŸ­çš„é”™è¯¯ç­”æ¡ˆï¼Œå½“ä¼˜åŠ¿ä¸ºè´Ÿçš„æ—¶å€™ï¼Œä»–ä½¿è¾ƒé•¿çš„é”™è¯¯å›ç­”å—åˆ°æ›´å°çš„æƒ©ç½šï¼Œä»è€Œé¼“åŠ±æ¨¡å‹èƒ¡è¨€ä¹±è¯­ï¼Œç”Ÿæˆæ›´é•¿çš„é”™è¯¯å›ç­”ã€‚  \n",
    "2ã€é—®é¢˜çº§åˆ«çš„éš¾åº¦åå·®ï¼Œæºäºç›®æ ‡å‡½æ•°ä¸­çš„1/stdï¼Œå¯¹äºé‚£äº›æ‰€æœ‰å›ç­”éƒ½æ­£ç¡®ï¼ˆéå¸¸ç®€å•ï¼‰æˆ–éƒ½é”™è¯¯ï¼ˆéå¸¸å›°éš¾ï¼‰çš„é—®é¢˜ï¼Œç»„å†…å¥–åŠ±çš„æ ‡å‡†å·®ä¼šéå¸¸å°ï¼Œæ¥è¿‘äºé›¶ã€‚è¿™ä¼šå¯¼è‡´ä¼˜åŠ¿å€¼è¢«æå¤§åœ°æ”¾å¤§ï¼Œä½¿å¾—è¿™äº›ç®€å•æˆ–å›°éš¾çš„é—®é¢˜åœ¨æ¢¯åº¦æ›´æ–°ä¸­å æ®è¿‡å¤§çš„æƒé‡ï¼Œä»è€Œäº§ç”Ÿå¯¹ç‰¹å®šéš¾åº¦çš„åå¥½ã€‚  \n",
    "\n",
    "è§£å†³æ–¹æ¡ˆ: Dr. GRPO (\"Doctor\" GRPO) çš„è§£å†³æ–¹æ¡ˆéå¸¸ç›´æ¥ï¼šç§»é™¤è¿™ä¸¤ä¸ªå¯¼è‡´åå·®çš„å½’ä¸€åŒ–é¡¹ã€‚å³ï¼Œåœ¨è®¡ç®—ç­–ç•¥æ¢¯åº¦æ—¶ä¸é™¤ä»¥å›ç­”é•¿åº¦ ï¼Œåœ¨è®¡ç®—ä¼˜åŠ¿æ—¶ä¸é™¤ä»¥å¥–åŠ±çš„æ ‡å‡†å·®ã€‚ä¿®æ­£åçš„æ— åç­–ç•¥æ¢¯åº¦æ›´æ¥è¿‘äºæ ‡å‡†çš„PPOç›®æ ‡ï¼Œå…¶ä¼˜åŠ¿é¡¹ç®€åŒ–ä¸ºä¸­å¿ƒåŒ–çš„å›æŠ¥  \n",
    "\n",
    "å®ƒæ”¾å¼ƒäº†ä¼˜åŠ¿æ ‡å‡†åŒ–å¸¦æ¥çš„æ¢¯åº¦ç¨³å®šæ€§ï¼Œä»¥æ¢å–å¯¹æ‰€æœ‰éš¾åº¦é—®é¢˜ä¸€è§†åŒä»çš„æ— åå­¦ä¹ ä¿¡å·ã€‚  \n",
    "\n",
    "è™½ç„¶æ–¹å·®å½’ä¸€åŒ–åœ¨é€šç”¨RLä¸­æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å·¥å…·ï¼Œä½†åœ¨ç‰¹å®šåº”ç”¨åœºæ™¯ä¸‹ï¼Œå®ƒå¯èƒ½å¼•å…¥ä¸æœŸæœ›çš„å½’çº³åç½®ã€‚  \n",
    "åç»­ç®—æ³•ï¼ˆå¦‚LitePPOï¼‰åˆ™é‡‡å–äº†æŠ˜è¡·æ–¹æ¡ˆï¼Œåœ¨æ›´å¤§çš„æ‰¹æ¬¡ï¼ˆbatchï¼‰çº§åˆ«ä¸Šè®¡ç®—æ–¹å·®ï¼Œä»¥è·å¾—æ›´é²æ£’çš„ä¼°è®¡  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880c649",
   "metadata": {},
   "source": [
    "# 5ã€GSPO\n",
    "åœ¨å¯¹å¤§è§„æ¨¡æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯MoEæ¶æ„ï¼‰è¿›è¡ŒGRPOè®­ç»ƒæ—¶ï¼Œè§‚å¯Ÿåˆ°äº†â€œç¾éš¾æ€§ä¸”ä¸å¯é€†çš„æ¨¡å‹å´©æºƒâ€ç°è±¡ï¼Œæ—¢ç„¶å¥–åŠ±æ˜¯æˆäºˆæ•´ä¸ªåºåˆ—çš„ï¼Œé‚£ä¹ˆé‡è¦æ€§é‡‡æ ·å’Œæˆªæ–­ä¹Ÿåº”è¯¥åœ¨åºåˆ—ï¼ˆsequenceï¼‰çº§åˆ«è¿›è¡Œã€‚GSPOå®šä¹‰äº†åºåˆ—çº§åˆ«çš„é‡è¦æ€§æ¯”ç‡ ï¼Œå¹¶å¯¹å…¶è¿›è¡Œé•¿åº¦å½’ä¸€åŒ–ä»¥å‡å°æ–¹å·®ï¼Œå…¶ç›®æ ‡å‡½æ•°ç›¸åº”åœ°å˜ä¸ºåœ¨åºåˆ—çº§åˆ«è¿›è¡Œæˆªæ–­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1896bb",
   "metadata": {},
   "source": [
    "# 6ã€GMPO\n",
    "é—®é¢˜è¯Šæ–­: GRPOçš„ç›®æ ‡å‡½æ•°æœ¬è´¨ä¸Šæ˜¯åœ¨ä¼˜åŒ–tokençº§åˆ«é‡è¦æ€§åŠ æƒå¥–åŠ±çš„ç®—æœ¯å¹³å‡å€¼ã€‚ç®—æœ¯å¹³å‡å€¼çš„ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„å¼±ç‚¹æ˜¯å®ƒå¯¹å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰éå¸¸æ•æ„Ÿã€‚åœ¨RLè®­ç»ƒä¸­ï¼ŒæŸäº›tokenå¯èƒ½ä¼šå› ä¸ºç­–ç•¥çš„å‰§çƒˆæ³¢åŠ¨è€Œäº§ç”Ÿæç«¯çš„é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼Œè¿™äº›å¼‚å¸¸å€¼ä¼šä¸»å¯¼æ•´ä¸ªæ¢¯åº¦æ›´æ–°ï¼Œå¯¼è‡´ç­–ç•¥æ›´æ–°ä¸ç¨³å®šã€‚\n",
    "GMPO (Geometric-Mean Policy Optimization) æå‡ºç”¨å‡ ä½•å¹³å‡å€¼æ¥æ›¿ä»£ç®—æœ¯å¹³å‡å€¼ã€‚å‡ ä½•å¹³å‡å€¼åœ¨æ•°å­¦ä¸Šå¯¹æç«¯å¤§å€¼ä¸é‚£ä¹ˆæ•æ„Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e15caf",
   "metadata": {},
   "source": [
    "# 7ã€**DAPO**\n",
    "è¦è§£å†³å‡ ä¸ªåœ¨è§„æ¨¡åŒ–è®­ç»ƒçš„ä¸­å¸¸è§çš„æ£˜æ‰‹é—®é¢˜ï¼š\n",
    "- ç†µå´©æºƒï¼šåœ¨RLåˆå§‹é˜¶æ®µï¼Œç­–ç•¥çš„ç†µï¼ˆéšæœºæ€§ï¼‰å¯èƒ½ä¼šè¿…é€Ÿä¸‹é™ï¼Œå¯¼è‡´æ¨¡å‹è¿‡æ—©åœ°å˜å¾—ç¡®å®šåŒ–ï¼Œä»è€Œé™åˆ¶äº†æ¢ç´¢ï¼Œé™·å…¥å±€éƒ¨æœ€ä¼˜\n",
    "- å¥–åŠ±å™ªå£°ï¼šå¯¹äºå› è¶…å‡ºé•¿åº¦é™åˆ¶è€Œè¢«æˆªæ–­çš„å›ç­”ï¼Œç®€å•çš„å¥–åŠ±è®¾è®¡å¯èƒ½ä¼šæƒ©ç½šä¸€ä¸ªæœ¬åº”æ­£ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œå¼•å…¥å™ªå£°ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§\n",
    "- æ¢¯åº¦æ¶ˆå¤±ï¼šä¸€ä¸ªbatchä¸­æ‰€æœ‰çš„å›ç­”éƒ½æ­£ç¡®æˆ–è€…é”™è¯¯ï¼Œad=0ï¼Œè¿™æ ·çš„batchå¯¹äºpolicyçš„updateæ²¡æœ‰ä»»ä½•è´¡çŒ®\n",
    "\n",
    "è§£å†³æ–¹æ³•ï¼š  \n",
    "1ã€Clip-Higher:æˆªæ–­åŒºé—´çš„ä¸Šä¸‹ç•Œé™è§£è€¦ï¼Œè®¾ç½®ä¸€ä¸ªæ¯”ä¸‹ç•Œæ›´é«˜çš„ä¸Šç•Œï¼Œå…è®¸æ­£å‘æ›´æ–°æ›´å¤§å¹…åº¦åœ°å¢åŠ å¥½å›ç­”çš„æ¦‚ç‡ï¼Œä»è€Œå¢å¼ºäº†ç­–ç•¥çš„ç†µï¼Œä¿ƒè¿›äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§å’Œæ¢ç´¢ã€‚  \n",
    "2ã€Dynamic Sampling:æ•°æ®è¿‡æ»¤ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šä¸»åŠ¨é‡‡æ ·å¹¶ä¸”ä¸¢å¼ƒé‚£äº›æ‰€æœ‰å›ç­”éƒ½æ­£ç¡®æˆ–è€…é”™è¯¯çš„ç»„ï¼Œï¼Œç¡®ä¿æ¯ä¸ªç”¨äºæ¢¯åº¦è®¡ç®—batchéƒ½åŒ…å«äº†æ­£ç¡®å’Œé”™è¯¯çš„æ ·æœ¬ã€‚æ ¹æ®å½“å‰ç­–ç•¥çš„æ€§èƒ½åŠ¨æ€è°ƒæ•´é‡‡æ ·çš„å›ç­”æ•°é‡Gã€‚åœ¨è®­ç»ƒåˆæœŸï¼Œä½¿ç”¨è¾ƒå¤§çš„Gä»¥å¢åŠ å¤šæ ·æ€§å’Œæ¢ç´¢ï¼›éšç€è®­ç»ƒè¿›å±•ï¼Œé€æ¸å‡å°Gä»¥é›†ä¸­è®¡ç®—èµ„æºåœ¨æ›´æœ‰ä»·å€¼çš„æ ·æœ¬ä¸Šï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ã€‚  \n",
    "3ã€token-level Policyï¼šåŸå§‹çš„GRPOæŸå¤±åœ¨åºåˆ—çº§åˆ«è¿›è¡Œå¹³å‡ï¼Œè¿™ä¼šå¯¼è‡´é•¿åºåˆ—ä¸­çš„æ¯ä¸ªtokenå¯¹æ€»æŸå¤±çš„è´¡çŒ®è¢«ç¨€é‡Šã€‚åˆ†æ¯æ”¹æˆæ‰¹æ¬¡å†…æ‰€æœ‰tokençš„æ€»æ•°\n",
    "4ã€Overlong Reward Shapingï¼šåœ¨è®­ç»ƒä¸­å±è”½æ‰è¢«æˆªæ ·æœ¬çš„æŸå¤±è®¡ç®—ï¼Œä»è€Œé¿å…äº†ç”±äºå¥–åŠ±å™ªå£°å¼•èµ·çš„è®­ç»ƒä¸ç¨³å®šæ€§ã€‚\n",
    "5ã€soft Overlong Punishment:å¯¹äºè¢«æˆªæ–­çš„å›ç­”ï¼Œä¸æ˜¯ç®€å•åœ°å°†å¥–åŠ±è®¾ä¸º0ï¼Œè€Œæ˜¯æ ¹æ®å›ç­”çš„å®é™…é•¿åº¦ç»™äºˆä¸€ä¸ªé€’å‡çš„å¥–åŠ±ã€‚è¿™ç§æ–¹å¼èƒ½å¤Ÿæ›´ç»†ç²’åº¦åœ°åæ˜ å›ç­”çš„è´¨é‡ï¼Œå‡å°‘äº†ç”±äºæˆªæ–­å¼•å…¥çš„å™ªå£°å¯¹è®­ç»ƒçš„å½±å“ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd65a63",
   "metadata": {},
   "source": [
    "# 8ã€**DPO**\n",
    "DPO (Direct Preference Optimization) æå‡ºäº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥ä¼˜åŒ–åå¥½æ•°æ®æ¥è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼Œè€Œä¸ä¾èµ–äºä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚DPOçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†åå¥½æ•°æ®è½¬åŒ–ä¸ºä¸€ä¸ªå¯¹æ•°å‡ ç‡æ¯”ï¼ˆlog-odds ratioï¼‰ï¼Œå¹¶é€šè¿‡æœ€å¤§åŒ–è¿™ä¸ªæ¯”ç‡æ¥ä¼˜åŒ–ç­–ç•¥ã€‚\n",
    "DPOçš„ç›®æ ‡å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n",
    "$$\n",
    "L^{DPO}(\\theta)\n",
    "=\n",
    "- \\mathbb{E}_{(x, y^+, y^-) \\sim D}\n",
    "\\left[\n",
    "\\log \\sigma \\left(\\frac{1}{\\beta} \\left( \\log \\pi_\\theta(y^+ \\mid x) - \\log \\pi_\\theta(y^- \\mid x) \\right)\\right)\n",
    "\\right]\n",
    "$$\n",
    "- å…¶ä¸­ï¼Œ$(x, y^+, y^-)$ è¡¨ç¤ºä¸€ä¸ªåå¥½å¯¹\n",
    "- $\\sigma$ æ˜¯sigmoidå‡½æ•°\n",
    "- $\\beta$ æ˜¯ä¸€ä¸ªæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åå¥½å¼ºåº¦\n",
    "\n",
    "sigmoidå‡½æ•°å°†å¯¹æ•°å‡ ç‡æ¯”æ˜ å°„åˆ°0åˆ°1ä¹‹é—´ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹åå¥½å¯¹çš„é¢„æµ‹æ¦‚ç‡ã€‚é€šè¿‡æœ€å¤§åŒ–è¿™ä¸ªæ¦‚ç‡ï¼ŒDPOç›´æ¥ä¼˜åŒ–æ¨¡å‹ä»¥ç¬¦åˆäººç±»çš„åå¥½ï¼Œè€Œä¸éœ€è¦æ˜¾å¼åœ°å»ºæ¨¡å¥–åŠ±å‡½æ•°æˆ–ä»·å€¼å‡½æ•°ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
